{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!conda install -y gdown","metadata":{"execution":{"iopub.status.busy":"2023-02-19T16:40:23.159577Z","iopub.execute_input":"2023-02-19T16:40:23.161078Z","iopub.status.idle":"2023-02-19T16:44:32.124998Z","shell.execute_reply.started":"2023-02-19T16:40:23.161023Z","shell.execute_reply":"2023-02-19T16:44:32.123726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown --id 14mto87jQZLt4hTOcVV1H3wIyF1cvS4fS","metadata":{"execution":{"iopub.status.busy":"2023-02-19T16:44:32.128012Z","iopub.execute_input":"2023-02-19T16:44:32.128446Z","iopub.status.idle":"2023-02-19T16:44:37.199454Z","shell.execute_reply.started":"2023-02-19T16:44:32.128413Z","shell.execute_reply":"2023-02-19T16:44:37.197724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Dropout, Dense,Input,Embedding,Flatten, MaxPooling1D, Conv1D\nfrom keras.models import Sequential,Model\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom sklearn import metrics\nfrom keras.preprocessing.text import Tokenizer\nfrom keras_preprocessing.sequence import pad_sequences\nfrom sklearn.datasets import fetch_20newsgroups\nfrom keras.layers import Concatenate","metadata":{"execution":{"iopub.status.busy":"2023-02-19T16:44:37.201446Z","iopub.execute_input":"2023-02-19T16:44:37.201919Z","iopub.status.idle":"2023-02-19T16:44:37.210270Z","shell.execute_reply.started":"2023-02-19T16:44:37.201879Z","shell.execute_reply":"2023-02-19T16:44:37.208871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newsgroups_train = fetch_20newsgroups(subset='train')\nnewsgroups_test = fetch_20newsgroups(subset='test')\nX_train = newsgroups_train.data\nX_test = newsgroups_test.data\ny_train = newsgroups_train.target\ny_test = newsgroups_test.target","metadata":{"execution":{"iopub.status.busy":"2023-02-19T16:44:37.213257Z","iopub.execute_input":"2023-02-19T16:44:37.213632Z","iopub.status.idle":"2023-02-19T16:44:37.911459Z","shell.execute_reply.started":"2023-02-19T16:44:37.213602Z","shell.execute_reply":"2023-02-19T16:44:37.910167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n    np.random.seed(7)\n    text = np.concatenate((X_train, X_test), axis=0)\n    text = np.array(text)\n    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n    tokenizer.fit_on_texts(text)\n    sequences = tokenizer.texts_to_sequences(text)\n    word_index = tokenizer.word_index\n    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n    print('Found %s unique tokens.' % len(word_index))\n    indices = np.arange(text.shape[0])\n    # np.random.shuffle(indices)\n    text = text[indices]\n    print(text.shape)\n    X_train = text[0:len(X_train), ]\n    X_test = text[len(X_train):, ]\n    embeddings_index = {}\n    f = open(\"/kaggle/working/glove.6B.50d.txt\", encoding=\"utf8\")\n    for line in f:\n        values = line.split()\n        word = values[0]\n        try:\n            coefs = np.asarray(values[1:], dtype='float32')\n        except:\n            pass\n        embeddings_index[word] = coefs\n    f.close()\n    print('Total %s word vectors.' % len(embeddings_index))\n    return (X_train, X_test, word_index,embeddings_index)","metadata":{"execution":{"iopub.status.busy":"2023-02-19T16:44:37.913010Z","iopub.execute_input":"2023-02-19T16:44:37.913436Z","iopub.status.idle":"2023-02-19T16:44:37.924578Z","shell.execute_reply.started":"2023-02-19T16:44:37.913401Z","shell.execute_reply":"2023-02-19T16:44:37.923310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)","metadata":{"execution":{"iopub.status.busy":"2023-02-19T16:44:37.926424Z","iopub.execute_input":"2023-02-19T16:44:37.926957Z","iopub.status.idle":"2023-02-19T16:45:57.524480Z","shell.execute_reply.started":"2023-02-19T16:44:37.926911Z","shell.execute_reply":"2023-02-19T16:45:57.523166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Build_Model_CNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n    \"\"\"\n        def buildModel_CNN(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n        word_index in word index ,\n        embeddings_index is embeddings index, look at data_helper.py\n        nClasses is number of classes,\n        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n    \"\"\"\n    model = Sequential()\n    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            if len(embedding_matrix[i]) !=len(embedding_vector):\n                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n                exit(1)\n            embedding_matrix[i] = embedding_vector\n    embedding_layer = Embedding(len(word_index) + 1,\n                                EMBEDDING_DIM,\n                                weights=[embedding_matrix],\n                                input_length=MAX_SEQUENCE_LENGTH,\n                                trainable=True)\n    # applying a more complex convolutional approach\n    convs = []\n    filter_sizes = []\n    layer = 5\n    print(\"Filter  \",layer)\n    for fl in range(0,layer):\n        filter_sizes.append((fl+2))\n    node = 128\n    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n    embedded_sequences = embedding_layer(sequence_input)\n    for fsz in filter_sizes:\n        l_conv = Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)\n        l_pool = MaxPooling1D(5)(l_conv)\n        #l_pool = Dropout(0.25)(l_pool)\n        convs.append(l_pool)\n    l_merge = Concatenate(axis=1)(convs)\n    l_cov1 = Conv1D(node, 5, activation='relu')(l_merge)\n    l_cov1 = Dropout(dropout)(l_cov1)\n    l_pool1 = MaxPooling1D(5)(l_cov1)\n    l_cov2 = Conv1D(node, 5, activation='relu')(l_pool1)\n    l_cov2 = Dropout(dropout)(l_cov2)\n    l_pool2 = MaxPooling1D(30)(l_cov2)\n    l_flat = Flatten()(l_pool2)\n    l_dense = Dense(1024, activation='relu')(l_flat)\n    l_dense = Dropout(dropout)(l_dense)\n    l_dense = Dense(512, activation='relu')(l_dense)\n    l_dense = Dropout(dropout)(l_dense)\n    preds = Dense(nclasses, activation='softmax')(l_dense)\n    model = Model(sequence_input, preds)\n    model.compile(loss='sparse_categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-02-19T16:45:57.527584Z","iopub.execute_input":"2023-02-19T16:45:57.528075Z","iopub.status.idle":"2023-02-19T16:45:57.547815Z","shell.execute_reply.started":"2023-02-19T16:45:57.528027Z","shell.execute_reply":"2023-02-19T16:45:57.545806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_CNN = Build_Model_CNN_Text(word_index,embeddings_index, 20)\nmodel_CNN.summary()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T16:45:57.549175Z","iopub.execute_input":"2023-02-19T16:45:57.549538Z","iopub.status.idle":"2023-02-19T16:45:58.491721Z","shell.execute_reply.started":"2023-02-19T16:45:57.549508Z","shell.execute_reply":"2023-02-19T16:45:58.490396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_CNN.fit(X_train_Glove, y_train,\n                              validation_data=(X_test_Glove, y_test),\n                              epochs=15,\n                              batch_size=128,\n                              verbose=2)","metadata":{"execution":{"iopub.status.busy":"2023-02-19T16:45:58.493415Z","iopub.execute_input":"2023-02-19T16:45:58.493790Z","iopub.status.idle":"2023-02-19T17:21:19.394073Z","shell.execute_reply.started":"2023-02-19T16:45:58.493757Z","shell.execute_reply":"2023-02-19T17:21:19.392392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = model_CNN.predict(X_test_Glove)\npredicted = np.argmax(predicted, axis=1)\nprint(metrics.classification_report(y_test, predicted))","metadata":{"execution":{"iopub.status.busy":"2023-02-19T17:21:19.399475Z","iopub.execute_input":"2023-02-19T17:21:19.399861Z","iopub.status.idle":"2023-02-19T17:21:40.184096Z","shell.execute_reply.started":"2023-02-19T17:21:19.399828Z","shell.execute_reply":"2023-02-19T17:21:40.182791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}